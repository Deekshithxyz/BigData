{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataGrokr.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deekshithxyz/BigData/blob/main/IPLProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "VUj4UnPStLli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "xRkzskJiuHjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "D7au8PbFuTmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6b67f1-01af-4382-a0ed-94ba909719c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "xnNpamdvuacs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "gckDDovzur5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark"
      ],
      "metadata": {
        "id": "T76so-9MuvW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.init()"
      ],
      "metadata": {
        "id": "he8FB8Aeuzbk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ff91bfa-ad27-424d-b474-671ed1de592d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5f9243a09cec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    161\u001b[0m             raise Exception(\n\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[0;32m--> 163\u001b[0;31m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                 )\n\u001b[1;32m    165\u001b[0m             )\n",
            "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.1.2-bin-hadoop2.7/python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "xHpJEvjRu3eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, StringType "
      ],
      "metadata": {
        "id": "XcIaK387vcZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "PKR_0wVdmzEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (SparkSession\n",
        ".builder\n",
        ".appName(\"IPL\")\n",
        ".getOrCreate())"
      ],
      "metadata": {
        "id": "RMnosqUju9fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema_matches = StructType([StructField(\"match_id\",IntegerType(),True),StructField(\"date\",TimestampType(),True),StructField(\"player_of_match\",StringType(),True),StructField(\"venue_id\",IntegerType(),True),StructField(\"neutral_venue\",IntegerType(),True),StructField(\"team1\",StringType(),True),StructField(\"team2\",StringType(),True),StructField(\"toss_winner\",StringType(),True),StructField(\"toss_decision\",StringType(),True),StructField(\"winner\",StringType(),True),StructField(\"result\",StringType(),True),StructField(\"result_margin\",IntegerType(),True),StructField(\"eliminator\",StringType(),True),StructField(\"method\", StringType(),True),StructField(\"umpire1\", StringType(),True),StructField(\"umpire2\", StringType(),True)])"
      ],
      "metadata": {
        "id": "-bzjAcBewd1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matches = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").schema(schema_matches).load(\"ipl_matches.csv\")"
      ],
      "metadata": {
        "id": "Km1oXI79le7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "vj84nJ7IrG1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "venues = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"ipl_venue.csv\")"
      ],
      "metadata": {
        "id": "UKRZZeHLl4m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ball_by_ball = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"ipl_ball_by_ball.csv\")"
      ],
      "metadata": {
        "id": "IN6w7JiHme2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matches.createOrReplaceTempView(\"MATCH\")"
      ],
      "metadata": {
        "id": "YGr7PoWhm_Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "venues.createOrReplaceTempView(\"VENUE\")"
      ],
      "metadata": {
        "id": "6JdE4J9YB4EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ball_by_ball.createOrReplaceTempView(\"BALL\")"
      ],
      "metadata": {
        "id": "0iMINuTLOxIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = spark.sql(\"select VENUE.venue_id, VENUE.venue, VENUE.city, tvenue.count from VENUE inner join (select venue_id, count(match_id) as count from MATCH where eliminator='Y' group by venue_id order by count desc limit 3) as tvenue on VENUE.venue_id=tvenue.venue_id order by count desc\")"
      ],
      "metadata": {
        "id": "9rEjptTRPggG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = spark.sql(\"select count(fielder) as catches from BALL where dismissal_kind='caught' group by fielder order by catches desc limit 1\")"
      ],
      "metadata": {
        "id": "PC9HHfRdaC9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = spark.sql(\"select bowler, count(*) as wickets from BALL where is_wicket=1 and match_id in (select match_id from MATCH where method='D/L') group by bowler order by wickets desc limit 1\")"
      ],
      "metadata": {
        "id": "_4BHYJo0G1H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = spark.sql(\"select batsman, sum(batsman_runs)*100/count(*) as strike_rate  from BALL where overs >= 7 and not extras_type='wides' and not extras_type='noballs' group by batsman order by strike_rate desc limit 1\")"
      ],
      "metadata": {
        "id": "X8tdhPU4x6kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = spark.sql(\"select sum(A.extra_runs) as extra_runs, first(C.venue) as venue, first(C.city) as city from BALL as A inner join MATCH as B on A.match_id=B.match_id inner join VENUE as C on B.venue_id=C.venue_id group by C.venue_id order by extra_runs desc limit 1\")"
      ],
      "metadata": {
        "id": "5vLosbknQk0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = spark.sql(\"select * from (select player_of_match, count(*) as count from MATCH where neutral_venue=1 group by player_of_match) where count=(select max(count) from (select count(*) as count from MATCH where neutral_venue=1 group by player_of_match))\")"
      ],
      "metadata": {
        "id": "RoQHcwPO0uOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "player_dismissed=ball_by_ball.groupby('player_dismissed').count()"
      ],
      "metadata": {
        "id": "W5gslwYtJ7M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs=ball_by_ball.groupBy('batsman').sum('batsman_runs')"
      ],
      "metadata": {
        "id": "Vwlyo3Xkl-tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_data=player_dismissed.join(runs, player_dismissed.player_dismissed == runs.batsman, 'inner')"
      ],
      "metadata": {
        "id": "D9bPJolRmFfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AVG=joined_data.withColumn(\"Batsmen Avg\", col(\"sum(batsman_runs)\")/col(\"count\")).sort('Batsmen Avg',ascending=False)"
      ],
      "metadata": {
        "id": "hvAPa11XmULL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = AVG.select('batsman','Batsmen Avg')"
      ],
      "metadata": {
        "id": "gC0aHmrBoJDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = spark.sql(\"select umpire, count(*) as count from ((select umpire1 as umpire from MATCH) union all (select umpire2 from MATCH)) as matches group by umpire order by count desc\")"
      ],
      "metadata": {
        "id": "JRefHYnaSx3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = spark.sql(\"select B.match_id,first(C.venue),first(C.city),sum(A.batsman_runs) from BALL as A inner join MATCH as B on A.match_id=B.match_id inner join VENUE as C on B.venue_id =C.venue_id where A.batsman = 'V Kohli' group by B.match_id order by sum(A.batsman_runs) desc limit 1\")"
      ],
      "metadata": {
        "id": "EmqUcozCVNvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j = spark.sql(\"select sum(count) from (select winner, count(winner) as count from MATCH where toss_winner=winner group by winner)\")"
      ],
      "metadata": {
        "id": "XnX1h6XkjrFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from sqlite3 import Error"
      ],
      "metadata": {
        "id": "SLVtMjHRu_JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Database:\n",
        "\n",
        "   def __init__(self, m):\n",
        "    self.path = m\n",
        " \n",
        "   def create_connection(self):\n",
        "    connection = None\n",
        "    try:\n",
        "        connection = sqlite3.connect(self.path)\n",
        "        print(\"Connection to SQLite DB successful\")\n",
        "    except Error as e:\n",
        "        print(f\"The error '{e}' occurred\")\n",
        "    return connection\n",
        "\n",
        "   def execute_query(self, connect, query):\n",
        "    cursor = connect.cursor()\n",
        "    try:\n",
        "        cursor.execute(query)\n",
        "        connect.commit()\n",
        "        print(\"Query executed successfully\")\n",
        "    except Error as e:\n",
        "        print(f\"The error '{e}' occurred\")"
      ],
      "metadata": {
        "id": "IvCsdhfvqqHd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}